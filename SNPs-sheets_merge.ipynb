{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some pandas opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### where are the files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m16 documents from consensus.geneious\u001b[0m*\r\n",
      "\u001b[01;32m16 documents from WT-day70 to WT-zero point.geneious\u001b[0m*\r\n",
      "\u001b[34;42mBY-day0\u001b[0m/\r\n",
      "\u001b[34;42mBY-nup133-day70\u001b[0m/\r\n",
      "\u001b[34;42mBY-WT-day70\u001b[0m/\r\n",
      "\u001b[01;32mC2_merged_dfs.csv\u001b[0m*\r\n",
      "\u001b[34;42mconsensus\u001b[0m/\r\n",
      "\u001b[34;42mmapped_contigs\u001b[0m/\r\n",
      "\u001b[34;42mmapped_reads\u001b[0m/\r\n",
      "\u001b[01;32mnup mapped to wt cons.geneious\u001b[0m*\r\n",
      "\u001b[34;42mW303-cog7-day42\u001b[0m/\r\n",
      "\u001b[34;42mW303-nup133-day42\u001b[0m/\r\n",
      "\u001b[34;42mWT-day70\u001b[0m/\r\n",
      "\u001b[01;32mWT-day70.xlsx\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "%ls /home/dizak/Pulpit/BIONAS/G148/SNPs_calling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampling_levels = [\"C1\", \"C2\", \"C3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/W303-cog7-day42/C1/*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N2_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/W303-cog7-day42/C2/*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N3_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/W303-cog7-day42/C3/*csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get non-redundant list of genes in the inputfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_flat_value(inputfiles_list,\n",
    "                    col_name = \"CDS\"):\n",
    "    \"\"\"\n",
    "    Get flat list of desired values from list of CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    inputfiles_list: list of str\n",
    "        List of input CSV files.\n",
    "    col_name: str\n",
    "        Desired column name in the input CSV file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of desired values.\n",
    "    \"\"\"\n",
    "    values_list = []\n",
    "    for i in inputfiles_list:\n",
    "        df = pd.read_csv(i)\n",
    "        if len(df) == 0:\n",
    "            pass\n",
    "        elif col_name not in df.columns:\n",
    "            pass\n",
    "        else:\n",
    "            values_list.append(df[col_name].dropna().drop_duplicates().tolist())\n",
    "    return list(itertools.chain.from_iterable(values_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get values from input files by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_by_key(inputfiles_list,\n",
    "                key):\n",
    "    \"\"\"\n",
    "    Get pandas.DataFrame selected by a given key from list of CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    inputfiles_list: list of str\n",
    "        List of input CSV files.\n",
    "    key: str, int, float, bool\n",
    "        Key used as query against rows in the CSV files.\n",
    "    value_col: str\n",
    "        Column name which holds values to be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict of lists of desired values if pandas.Dataframe not empty\n",
    "    None if pandas.DataFrame empty\n",
    "    \"\"\"\n",
    "    values_list = []\n",
    "    for i in inputfiles_list:\n",
    "        filename = \"\".join(i.split(\"/\")[-1].split(\".\")[:-1])\n",
    "        df = pd.read_csv(i)\n",
    "        if len(df) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if isinstance(key, str) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"object\"])\n",
    "            elif isinstance(key, int) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"int\"])\n",
    "            elif isinstance(key, float) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"float\"])\n",
    "            elif isinstance(key, bool) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"bool\"])\n",
    "            else:\n",
    "                raise ValueError(\"key must str, int, float or bool dtype\")\n",
    "            for col in df_dtype_sel.columns:\n",
    "                df_sel = df[df_dtype_sel[col] == key]\n",
    "                if len(df_sel) > 0:\n",
    "                    return {\"dataframe\": df_sel,\n",
    "                            \"filename\": filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get whole set of pandas.DataFrames selections in one dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dfs_set(key_list,\n",
    "                files_list,\n",
    "                vals=[\"Minimum\",\n",
    "                      \"Maximum\",\n",
    "                      \"Change\"],\n",
    "                df_key=\"dataframe\",\n",
    "                key_index_name=\"Gene\",\n",
    "                smpl_index_name=\"Sample\",\n",
    "                row_index_name=\"Number\",\n",
    "                index_by_key=True,\n",
    "                smpl_index_val=None,):\n",
    "    \"\"\"\n",
    "    Get desired values in pandas.Dataframe gathered in dict by\n",
    "    the list of keys.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    key_list: list, tuple\n",
    "        List of keys by which pandas.Dataframes are\n",
    "        initially selected.\n",
    "    files_list: list, tuple\n",
    "        List of input CSV files.\n",
    "    vals: list of str, default: [\"Minimum\", \n",
    "                                 \"Maximum\",\n",
    "                                 \"Change\"]\n",
    "        Columns names holding data in pandas.DataFrames\n",
    "        selected.\n",
    "    df_key: str, default: <\"dataframe\">\n",
    "        Key for generic pandas.DataFrame selection for\n",
    "        SNPs-sheets_merge.find_by_key function.\n",
    "    key_index_name: str, default: <\"Gene\">\n",
    "        Name for index of selection key.\n",
    "    smpl_index_name: str, default: <\"Sample\">\n",
    "        Name for index of sample.\n",
    "    row_index_name: str, default: <\"Number\">\n",
    "        Name for numeric row index.\n",
    "    index_by_key: bool, default: True\n",
    "        Enables multiindexing.\n",
    "    smpl_index_val: str, default: <None>\n",
    "        Adds sample name to multiindexing if not <None>\n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for i in key_list:\n",
    "        key_vals = find_by_key(files_list,\n",
    "                               key=i)[df_key]\n",
    "        out_dict[i] = key_vals[vals]\n",
    "    if index_by_key is True:\n",
    "        for i in out_dict:\n",
    "            key_index = [i] * len(out_dict[i])\n",
    "            if smpl_index_val is not None:\n",
    "                smpl_index = [smpl_index_val] * len(out_dict[i])\n",
    "                tpls = list(zip(*[key_index,\n",
    "                                  smpl_index,\n",
    "                                  out_dict[i].index]))\n",
    "                mindex = pd.MultiIndex.from_tuples(tpls,\n",
    "                                                   names=[key_index_name,\n",
    "                                                          smpl_index_name,\n",
    "                                                          row_index_name])\n",
    "            else:\n",
    "                tpls = list(zip(*[key_index,\n",
    "                                  out_dict[i].index]))\n",
    "                mindex = pd.MultiIndex.from_tuples(tpls,\n",
    "                                                   names=[key_index_name,\n",
    "                                                          row_index_name])\n",
    "            out_dict[i].index = mindex\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge any given number of dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_dfs(dfs,\n",
    "              sort_cols=[\"Minimum\",\n",
    "                         \"Maximum\"],\n",
    "              reconstr_index=True):\n",
    "    \"\"\"\n",
    "    Merge any number of pandas.DataFrame into one.\n",
    "    Indexes must be identical in all the pandas.DataFrames.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    dfs: list\n",
    "        list of pandas.DataFrames to merge.\n",
    "    sort_cols: list, None\n",
    "        list of col names to sort the final\n",
    "        pandas.DataFrame by. No sorting if None.\n",
    "    \"\"\"\n",
    "    for x in [set(i.index) for i in dfs]:\n",
    "        assert len(x) == 1, \"Indices are not homogenic.\"\n",
    "    new_index = list(x)\n",
    "    df = reduce(lambda df1, df2: pd.merge(left=df1,\n",
    "                                          right=df2,\n",
    "                                          how=\"outer\"),\n",
    "                dfs)\n",
    "    if sort_cols is not None:\n",
    "        df.sort(columns=sort_cols)\n",
    "    if reconstr_index is True:\n",
    "        df.index = len(df) * new_index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's find out which CDS are present in all the files one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_CDSs = find_flat_value(strain_gene_day_N1_files)\n",
    "strain_gene_day_N2_CDSs = find_flat_value(strain_gene_day_N2_files)\n",
    "strain_gene_day_N3_CDSs = find_flat_value(strain_gene_day_N3_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's gather some info about each of the CDS from each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_dfs = get_dfs_set(strain_gene_day_N1_CDSs,\n",
    "                                     strain_gene_day_N1_files,\n",
    "                                     smpl_index_val=sampling_levels[0])\n",
    "strain_gene_day_N2_dfs = get_dfs_set(strain_gene_day_N2_CDSs,\n",
    "                                     strain_gene_day_N2_files,\n",
    "                                     smpl_index_val=sampling_levels[1])\n",
    "strain_gene_day_N3_dfs = get_dfs_set(strain_gene_day_N3_CDSs,\n",
    "                                     strain_gene_day_N3_files,\n",
    "                                     smpl_index_val=sampling_levels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's unwind them all from this dict into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C1_dfs = [strain_gene_day_N1_dfs[i] for i in strain_gene_day_N1_dfs.keys()]\n",
    "C2_dfs = [strain_gene_day_N2_dfs[i] for i in strain_gene_day_N2_dfs.keys()]\n",
    "C3_dfs = [strain_gene_day_N3_dfs[i] for i in strain_gene_day_N3_dfs.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's merge them to see changes between samples.\n",
    "#### rememeber now cannot use the merge_dfs function since multindex is NOT homogenic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C1_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "                C1_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C2_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "               C2_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C3_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "               C3_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C1_C2_C3_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "                     [C1_df, C2_df, C3_df])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
