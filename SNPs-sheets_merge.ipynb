{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pygsheets as pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from apiclient import discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from oauth2client.file import Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some pandas opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### secret file for auth for anything needed in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secret_json = \"./client_secret_986841527205-ndf6fbsfmrhjinnofm47j6olvmm11smn.apps.googleusercontent.com.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auth for drive. quick start script that takes secret client json file and creates conctents of\n",
    "#### ```~/.credentials```\n",
    "#### is needed!!!\n",
    "#### it will NOT work inside the notebook since here the argparse Namespace is broken! it is needed for the flags var (see quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credential_path = \"/home/dizak/.credentials/drive-python-quickstart.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = Storage(credential_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credentials = store.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "http = credentials.authorize(httplib2.Http())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = discovery.build(\"drive\", \"v3\", http=http)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mind if len(results[\"files\"]) > 1000 which is max per page you have to take care of pageToken.\n",
    "#### pageToken can be retrieved by \n",
    "#### ```res = service.files().list(pageSize=1000, fields=\"nextPageToken, files\").execute()```\n",
    "#### ```token = res[\"nextPageToken\"]```\n",
    "#### and then passed as arg like:\n",
    "#### ```service.files().list(pageSize=1000, pageToken=token, fields=\"nextPageToken, files\").execute()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = service.files().list(pageSize=1000).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[\"files\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### authorize pyg and set retry limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gc = pyg.authorize(outh_file=secret_json,\n",
    "                   outh_nonlocal=True,\n",
    "                   retries=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title for the google sheet and xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheet_name = \"BY4741-WT-day70\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xls output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xls_out_dir = \"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-WT-day70/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gdrive output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gdrive_out_dir = [i[\"id\"] for i in results[\"files\"] if i[\"name\"] == \"testing\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0BwDD9bCCIcilTkMwNGNjWl9vVXc'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdrive_out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naming of the sampling levels. it is INDEPENDENT of the actual naming in the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampling_levels = [\"N1\", \"N2\", \"N3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns names for by which sorting will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort_vals = [\"Minimum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSVs paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/WT-day70/N1/*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N2_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-WT-day70/BY-WT2-day70/*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N3_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-WT-day70/BY-WT3-day70/*csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get non-redundant list of genes in the inputfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_flat_value(inputfiles_list,\n",
    "                    col_name = \"CDS\"):\n",
    "    \"\"\"\n",
    "    Get flat list of desired values from list of CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    inputfiles_list: list of str\n",
    "        List of input CSV files.\n",
    "    col_name: str\n",
    "        Desired column name in the input CSV file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of desired values.\n",
    "    \"\"\"\n",
    "    values_list = []\n",
    "    for i in inputfiles_list:\n",
    "        df = pd.read_csv(i)\n",
    "        if len(df) == 0:\n",
    "            pass\n",
    "        elif col_name not in df.columns:\n",
    "            pass\n",
    "        else:\n",
    "            values_list.append(df[col_name].dropna().drop_duplicates().tolist())\n",
    "    return list(itertools.chain.from_iterable(values_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get values from input files by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_by_key(inputfiles_list,\n",
    "                key):\n",
    "    \"\"\"\n",
    "    Get pandas.DataFrame selected by a given key from list of CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    inputfiles_list: list of str\n",
    "        List of input CSV files.\n",
    "    key: str, int, float, bool\n",
    "        Key used as query against rows in the CSV files.\n",
    "    value_col: str\n",
    "        Column name which holds values to be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict of lists of desired values if pandas.Dataframe not empty\n",
    "    None if pandas.DataFrame empty\n",
    "    \"\"\"\n",
    "    values_list = []\n",
    "    for i in inputfiles_list:\n",
    "        filename = \"\".join(i.split(\"/\")[-1].split(\".\")[:-1])\n",
    "        df = pd.read_csv(i)\n",
    "        if len(df) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if isinstance(key, str) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"object\"])\n",
    "            elif isinstance(key, int) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"int\"])\n",
    "            elif isinstance(key, float) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"float\"])\n",
    "            elif isinstance(key, bool) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"bool\"])\n",
    "            else:\n",
    "                raise ValueError(\"key must str, int, float or bool dtype\")\n",
    "            for col in df_dtype_sel.columns:\n",
    "                df_sel = df[df_dtype_sel[col] == key]\n",
    "                if len(df_sel) > 0:\n",
    "                    return {\"dataframe\": df_sel,\n",
    "                            \"filename\": filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get whole set of pandas.DataFrames selections in one dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dfs_set(key_list,\n",
    "                files_list,\n",
    "                vals=[\"Minimum\",\n",
    "                      \"Maximum\",\n",
    "                      \"Change\"],\n",
    "                df_key=\"dataframe\",\n",
    "                key_index_name=\"Gene\",\n",
    "                smpl_index_name=\"Sample\",\n",
    "                row_index_name=\"Number\",\n",
    "                index_by_key=True,\n",
    "                smpl_index_val=None,):\n",
    "    \"\"\"\n",
    "    Get desired values in pandas.Dataframe gathered in dict by\n",
    "    the list of keys.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    key_list: list, tuple\n",
    "        List of keys by which pandas.Dataframes are\n",
    "        initially selected.\n",
    "    files_list: list, tuple\n",
    "        List of input CSV files.\n",
    "    vals: list of str, default: [\"Minimum\", \n",
    "                                 \"Maximum\",\n",
    "                                 \"Change\"]\n",
    "        Columns names holding data in pandas.DataFrames\n",
    "        selected.\n",
    "    df_key: str, default: <\"dataframe\">\n",
    "        Key for generic pandas.DataFrame selection for\n",
    "        SNPs-sheets_merge.find_by_key function.\n",
    "    key_index_name: str, default: <\"Gene\">\n",
    "        Name for index of selection key.\n",
    "    smpl_index_name: str, default: <\"Sample\">\n",
    "        Name for index of sample.\n",
    "    row_index_name: str, default: <\"Number\">\n",
    "        Name for numeric row index.\n",
    "    index_by_key: bool, default: True\n",
    "        Enables multiindexing.\n",
    "    smpl_index_val: str, default: <None>\n",
    "        Adds sample name to multiindexing if not <None>\n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for i in key_list:\n",
    "        key_vals = find_by_key(files_list,\n",
    "                               key=i)[df_key]\n",
    "        out_dict[i] = key_vals[vals]\n",
    "    if index_by_key is True:\n",
    "        for i in out_dict:\n",
    "            key_index = [i] * len(out_dict[i])\n",
    "            if smpl_index_val is not None:\n",
    "                smpl_index = [smpl_index_val] * len(out_dict[i])\n",
    "                tpls = list(zip(*[key_index,\n",
    "                                  smpl_index,\n",
    "                                  out_dict[i].index]))\n",
    "                mindex = pd.MultiIndex.from_tuples(tpls,\n",
    "                                                   names=[key_index_name,\n",
    "                                                          smpl_index_name,\n",
    "                                                          row_index_name])\n",
    "            else:\n",
    "                tpls = list(zip(*[key_index,\n",
    "                                  out_dict[i].index]))\n",
    "                mindex = pd.MultiIndex.from_tuples(tpls,\n",
    "                                                   names=[key_index_name,\n",
    "                                                          row_index_name])\n",
    "            out_dict[i].index = mindex\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge any given number of dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_dfs(dfs,\n",
    "              sort_cols=[\"Minimum\",\n",
    "                         \"Maximum\"],\n",
    "              reconstr_index=True):\n",
    "    \"\"\"\n",
    "    Merge any number of pandas.DataFrame into one.\n",
    "    Indexes must be identical in all the pandas.DataFrames.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    dfs: list\n",
    "        list of pandas.DataFrames to merge.\n",
    "    sort_cols: list, None\n",
    "        list of col names to sort the final\n",
    "        pandas.DataFrame by. No sorting if None.\n",
    "    reconstr_index: bool\n",
    "        Copy index from the original pandas.DataFrames\n",
    "        to the final one.\n",
    "    \"\"\"\n",
    "    for x in [set(i.index) for i in dfs]:\n",
    "        assert len(x) == 1, \"Indices are not homogenic.\"\n",
    "    new_index = list(x)\n",
    "    df = reduce(lambda df1, df2: pd.merge(left=df1,\n",
    "                                          right=df2,\n",
    "                                          how=\"outer\"),\n",
    "                dfs)\n",
    "    if sort_cols is not None:\n",
    "        df.sort(columns=sort_cols)\n",
    "    if reconstr_index is True:\n",
    "        df.index = len(df) * new_index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's find out which CDS are present in all the files one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_CDSs = find_flat_value(strain_gene_day_N1_files)\n",
    "strain_gene_day_N2_CDSs = find_flat_value(strain_gene_day_N2_files)\n",
    "strain_gene_day_N3_CDSs = find_flat_value(strain_gene_day_N3_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's gather some info about each of the CDS from each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "des_cols = [\"Change\",\n",
    "            \"Protein Effect\",\n",
    "            \"Polymorphism Type\",\n",
    "            \"Variant Frequency\",\n",
    "            \"Average Quality\",\n",
    "            \"Minimum\",\n",
    "            \"Maximum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_dfs = get_dfs_set(strain_gene_day_N1_CDSs,\n",
    "                                     strain_gene_day_N1_files,\n",
    "                                     vals=des_cols,\n",
    "                                     smpl_index_val=sampling_levels[0])\n",
    "strain_gene_day_N2_dfs = get_dfs_set(strain_gene_day_N2_CDSs,\n",
    "                                     strain_gene_day_N2_files,\n",
    "                                     vals=des_cols,\n",
    "                                     smpl_index_val=sampling_levels[1])\n",
    "strain_gene_day_N3_dfs = get_dfs_set(strain_gene_day_N3_CDSs,\n",
    "                                     strain_gene_day_N3_files,\n",
    "                                     vals=des_cols,\n",
    "                                     smpl_index_val=sampling_levels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's unwind them all from this dict into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N1_dfs = [strain_gene_day_N1_dfs[i] for i in strain_gene_day_N1_dfs.keys()]\n",
    "N2_dfs = [strain_gene_day_N2_dfs[i] for i in strain_gene_day_N2_dfs.keys()]\n",
    "N3_dfs = [strain_gene_day_N3_dfs[i] for i in strain_gene_day_N3_dfs.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's merge them to see changes between samples.\n",
    "#### rememeber now cannot use the merge_dfs function since multindex is NOT homogenic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N1_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "               N1_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N2_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "               N2_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N3_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "               N3_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N1_N2_N3_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "                     [N1_df, N2_df, N3_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N1_N2_N3_df.sort_values(sort_vals, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's make xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(\"{}/{}.xls\".format(xls_out_dir, sheet_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in set(strain_gene_day_N1_CDSs +\n",
    "             strain_gene_day_N2_CDSs +\n",
    "             strain_gene_day_N3_CDSs):\n",
    "    N1_N2_N3_df.xs(i).to_excel(excel_writer=writer,\n",
    "                               sheet_name=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's make sheet on gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sh = gc.create(sheet_name, parent_id=gdrive_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in set(strain_gene_day_N1_CDSs +\n",
    "             strain_gene_day_N2_CDSs +\n",
    "             strain_gene_day_N3_CDSs):\n",
    "    time.sleep(3)\n",
    "    wks = sh.add_worksheet(i)\n",
    "    wks.set_dataframe(N1_N2_N3_df.xs(i),\n",
    "                      (1, 1),\n",
    "                      copy_index=True,\n",
    "                      fit=True)\n",
    "sh.del_worksheet(sh.worksheet_by_title(\"Sheet1\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
