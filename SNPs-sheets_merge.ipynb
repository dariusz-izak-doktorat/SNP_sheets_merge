{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pygsheets as pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from apiclient import discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from oauth2client.file import Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import httplib2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some pandas opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### secret file for auth for anything needed in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secret_json = \"./client_secret_986841527205-ndf6fbsfmrhjinnofm47j6olvmm11smn.apps.googleusercontent.com.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### auth for drive. quick start script that takes secret client json file and creates conctents of\n",
    "#### ```~/.credentials```\n",
    "#### is needed!!!\n",
    "#### it will NOT work inside the notebook since here the argparse Namespace is broken! it is needed for the flags var (see quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credential_path = \"/home/dizak/.credentials/drive-python-quickstart.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store = Storage(credential_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credentials = store.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http = credentials.authorize(httplib2.Http())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = discovery.build(\"drive\", \"v3\", http=http)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mind if len(results[\"files\"]) > 1000 which is max per page you have to take care of pageToken.\n",
    "#### pageToken can be retrieved by \n",
    "#### ```res = service.files().list(pageSize=1000, fields=\"nextPageToken, files\").execute()```\n",
    "#### ```token = res[\"nextPageToken\"]```\n",
    "#### and then passed as arg like:\n",
    "#### ```service.files().list(pageSize=1000, pageToken=token, fields=\"nextPageToken, files\").execute()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = service.files().list(pageSize=1000).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results[\"files\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### authorize pyg and set retry limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc = pyg.authorize(outh_file=secret_json,\n",
    "                   outh_nonlocal=True,\n",
    "                   retries=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### title for the google sheet and xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheet_name = \"BY4741-nup133-day70\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xls output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xls_out_dir = \"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-nup133-day70/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gdrive output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdrive_out_dir = [i[\"id\"] for i in results[\"files\"] if i[\"name\"] == \"testing\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0BwDD9bCCIcilTkMwNGNjWl9vVXc'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdrive_out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### naming of the sampling levels. it is INDEPENDENT of the actual naming in the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampling_levels = [\"N1\", \"N2\", \"N3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns names for by which sorting will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sort_vals = [\"Minimum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSVs paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-nup133-day70/N1/*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N2_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-nup133-day70/N2/*csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N3_files = glob(\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-nup133-day70/N3/*csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get non-redundant list of genes in the inputfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_flat_value(inputfiles_list,\n",
    "                    col_name = \"CDS\"):\n",
    "    \"\"\"\n",
    "    Get flat list of desired values from list of CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    inputfiles_list: list of str\n",
    "        List of input CSV files.\n",
    "    col_name: str\n",
    "        Desired column name in the input CSV file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of desired values.\n",
    "    \"\"\"\n",
    "    values_list = []\n",
    "    for i in inputfiles_list:\n",
    "        df = pd.read_csv(i)\n",
    "        if len(df) == 0:\n",
    "            pass\n",
    "        elif col_name not in df.columns:\n",
    "            pass\n",
    "        else:\n",
    "            values_list.append(df[col_name].dropna().drop_duplicates().tolist())\n",
    "    return list(itertools.chain.from_iterable(values_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get values from input files by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_by_key(inputfiles_list,\n",
    "                key):\n",
    "    \"\"\"\n",
    "    Get pandas.DataFrame selected by a given key from list of CSV files.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    inputfiles_list: list of str\n",
    "        List of input CSV files.\n",
    "    key: str, int, float, bool\n",
    "        Key used as query against rows in the CSV files.\n",
    "    value_col: str\n",
    "        Column name which holds values to be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict of lists of desired values if pandas.Dataframe not empty\n",
    "    None if pandas.DataFrame empty\n",
    "    \"\"\"\n",
    "    values_list = []\n",
    "    for i in inputfiles_list:\n",
    "        filename = \"\".join(i.split(\"/\")[-1].split(\".\")[:-1])\n",
    "        df = pd.read_csv(i)\n",
    "        if len(df) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if isinstance(key, str) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"object\"])\n",
    "            elif isinstance(key, int) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"int\"])\n",
    "            elif isinstance(key, float) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"float\"])\n",
    "            elif isinstance(key, bool) == True:\n",
    "                df_dtype_sel = df.select_dtypes(include=[\"bool\"])\n",
    "            else:\n",
    "                raise ValueError(\"key must str, int, float or bool dtype\")\n",
    "            for col in df_dtype_sel.columns:\n",
    "                df_sel = df[df_dtype_sel[col] == key]\n",
    "                if len(df_sel) > 0:\n",
    "                    return {\"dataframe\": df_sel,\n",
    "                            \"filename\": filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get whole set of pandas.DataFrames selections in one dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dfs_set(key_list,\n",
    "                files_list,\n",
    "                vals=[\"Minimum\",\n",
    "                      \"Maximum\",\n",
    "                      \"Change\"],\n",
    "                df_key=\"dataframe\",\n",
    "                key_index_name=\"Gene\",\n",
    "                smpl_index_name=\"Sample\",\n",
    "                row_index_name=\"Number\",\n",
    "                index_by_key=True,\n",
    "                smpl_index_val=None,):\n",
    "    \"\"\"\n",
    "    Get desired values in pandas.Dataframe gathered in dict by\n",
    "    the list of keys.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    key_list: list, tuple\n",
    "        List of keys by which pandas.Dataframes are\n",
    "        initially selected.\n",
    "    files_list: list, tuple\n",
    "        List of input CSV files.\n",
    "    vals: list of str, default: [\"Minimum\", \n",
    "                                 \"Maximum\",\n",
    "                                 \"Change\"]\n",
    "        Columns names holding data in pandas.DataFrames\n",
    "        selected.\n",
    "    df_key: str, default: <\"dataframe\">\n",
    "        Key for generic pandas.DataFrame selection for\n",
    "        SNPs-sheets_merge.find_by_key function.\n",
    "    key_index_name: str, default: <\"Gene\">\n",
    "        Name for index of selection key.\n",
    "    smpl_index_name: str, default: <\"Sample\">\n",
    "        Name for index of sample.\n",
    "    row_index_name: str, default: <\"Number\">\n",
    "        Name for numeric row index.\n",
    "    index_by_key: bool, default: True\n",
    "        Enables multiindexing.\n",
    "    smpl_index_val: str, default: <None>\n",
    "        Adds sample name to multiindexing if not <None>\n",
    "    \"\"\"\n",
    "    out_dict = {}\n",
    "    for i in key_list:\n",
    "        key_vals = find_by_key(files_list,\n",
    "                               key=i)[df_key]\n",
    "        out_dict[i] = key_vals[vals]\n",
    "    if index_by_key is True:\n",
    "        for i in out_dict:\n",
    "            key_index = [i] * len(out_dict[i])\n",
    "            if smpl_index_val is not None:\n",
    "                smpl_index = [smpl_index_val] * len(out_dict[i])\n",
    "                tpls = list(zip(*[key_index,\n",
    "                                  smpl_index,\n",
    "                                  out_dict[i].index]))\n",
    "                mindex = pd.MultiIndex.from_tuples(tpls,\n",
    "                                                   names=[key_index_name,\n",
    "                                                          smpl_index_name,\n",
    "                                                          row_index_name])\n",
    "            else:\n",
    "                tpls = list(zip(*[key_index,\n",
    "                                  out_dict[i].index]))\n",
    "                mindex = pd.MultiIndex.from_tuples(tpls,\n",
    "                                                   names=[key_index_name,\n",
    "                                                          row_index_name])\n",
    "            out_dict[i].index = mindex\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge any given number of dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dfs(dfs,\n",
    "              sort_cols=[\"Minimum\",\n",
    "                         \"Maximum\"],\n",
    "              reconstr_index=True):\n",
    "    \"\"\"\n",
    "    Merge any number of pandas.DataFrame into one.\n",
    "    Indexes must be identical in all the pandas.DataFrames.\n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    dfs: list\n",
    "        list of pandas.DataFrames to merge.\n",
    "    sort_cols: list, None\n",
    "        list of col names to sort the final\n",
    "        pandas.DataFrame by. No sorting if None.\n",
    "    reconstr_index: bool\n",
    "        Copy index from the original pandas.DataFrames\n",
    "        to the final one.\n",
    "    \"\"\"\n",
    "    for x in [set(i.index) for i in dfs]:\n",
    "        assert len(x) == 1, \"Indices are not homogenic.\"\n",
    "    new_index = list(x)\n",
    "    df = reduce(lambda df1, df2: pd.merge(left=df1,\n",
    "                                          right=df2,\n",
    "                                          how=\"outer\"),\n",
    "                dfs)\n",
    "    if sort_cols is not None:\n",
    "        df.sort(columns=sort_cols)\n",
    "    if reconstr_index is True:\n",
    "        df.index = len(df) * new_index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's find out which CDS are present in all the files one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_CDSs = find_flat_value(strain_gene_day_N1_files)\n",
    "strain_gene_day_N2_CDSs = find_flat_value(strain_gene_day_N2_files)\n",
    "strain_gene_day_N3_CDSs = find_flat_value(strain_gene_day_N3_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's gather some info about each of the CDS from each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "des_cols = [\"Change\",\n",
    "            \"Protein Effect\",\n",
    "            \"Polymorphism Type\",\n",
    "            \"Variant Frequency\",\n",
    "            \"Average Quality\",\n",
    "            \"Minimum\",\n",
    "            \"Maximum\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strain_gene_day_N1_dfs = get_dfs_set(strain_gene_day_N1_CDSs,\n",
    "                                     strain_gene_day_N1_files,\n",
    "                                     vals=des_cols,\n",
    "                                     smpl_index_val=sampling_levels[0])\n",
    "strain_gene_day_N2_dfs = get_dfs_set(strain_gene_day_N2_CDSs,\n",
    "                                     strain_gene_day_N2_files,\n",
    "                                     vals=des_cols,\n",
    "                                     smpl_index_val=sampling_levels[1])\n",
    "strain_gene_day_N3_dfs = get_dfs_set(strain_gene_day_N3_CDSs,\n",
    "                                     strain_gene_day_N3_files,\n",
    "                                     vals=des_cols,\n",
    "                                     smpl_index_val=sampling_levels[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's unwind them all from this dict into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N1_dfs = [strain_gene_day_N1_dfs[i] for i in strain_gene_day_N1_dfs.keys()]\n",
    "N2_dfs = [strain_gene_day_N2_dfs[i] for i in strain_gene_day_N2_dfs.keys()]\n",
    "N3_dfs = [strain_gene_day_N3_dfs[i] for i in strain_gene_day_N3_dfs.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's merge them to see changes between samples.\n",
    "#### rememeber now cannot use the merge_dfs function since multindex is NOT homogenic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N1_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "               N1_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N2_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "               N2_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N3_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "               N3_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N1_N2_N3_df = reduce(lambda df1, df2: pd.concat([df1, df2]),\n",
    "                     [N1_df, N2_df, N3_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now let's clean this messy Variant Frequency col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N1_N2_N3_df[\"Variant Frequency\"] = N1_N2_N3_df[\"Variant Frequency\"].replace(\"%\", \"\", regex=True).\\\n",
    "                                                                    replace(\".+->\", \"\", regex=True).\\\n",
    "                                                                    astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and sort everything by the Minimum value so that repeatitions can be easily spotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N1_N2_N3_df.sort_values(sort_vals, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's make xls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "writer = pd.ExcelWriter(\"{}/{}.xls\".format(xls_out_dir, sheet_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in set(strain_gene_day_N1_CDSs +\n",
    "             strain_gene_day_N2_CDSs +\n",
    "             strain_gene_day_N3_CDSs):\n",
    "    N1_N2_N3_df.xs(i).to_excel(excel_writer=writer,\n",
    "                               sheet_name=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's make sheet on gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sh = gc.create(sheet_name, parent_id=gdrive_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "for i in set(strain_gene_day_N1_CDSs +\n",
    "             strain_gene_day_N2_CDSs +\n",
    "             strain_gene_day_N3_CDSs):\n",
    "    time.sleep(3)\n",
    "    wks = sh.add_worksheet(i)\n",
    "    wks.set_dataframe(N1_N2_N3_df.xs(i),\n",
    "                      (1, 1),\n",
    "                      copy_index=True,\n",
    "                      fit=True)\n",
    "sh.del_worksheet(sh.worksheet_by_title(\"Sheet1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional saving DataFrame to CSV without dividing it to worksheets,\n",
    "## so that it can be used for merging everything into one, unintelligible mess..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N1_N2_N3_df.to_csv(\"{}/{}.csv\".format(xls_out_dir, sheet_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this part of the notebook should be treated separately!\n",
    "## it is ment to be used for merging ALL previusly created sheets into ONE\n",
    "## since it is so, vars needed for this purpose are defined here instead of being on the TOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's read-in previously saved csv into dict, filenames as keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_list = [\"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/W303-WT-day42/W303-WT-day42.csv\",\n",
    "            \"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/W303-cog7-day42/W303-cog7-day42.csv\",\n",
    "            \"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/W303-nup133-day42/W303-nup133-day42.csv\",\n",
    "            \"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-WT-day70/BY4741-WT-day70.csv\",\n",
    "            \"/home/dizak/Pulpit/BIONAS/G148/SNPs_calling/BY-nup133-day70/BY4741-nup133-day70.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {i.split(\"/\")[-1].replace(\".csv\", \"\"): pd.read_csv(i, index_col=[\"Gene\",\n",
    "                                                                       \"Sample\",\n",
    "                                                                       \"Number\"]) for i in csv_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's add key of each DataFrame - which is its strain-deletion-day name - as column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in dfs.keys():\n",
    "    dfs[i][\"Strain\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's reset indices in each DataFrame to regular columns and get rid of the Number col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in dfs.values():\n",
    "    i.reset_index(level=[0, 1, 2], inplace=True)\n",
    "    i.drop(columns=\"Number\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns on which merge will take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_cols = [\"Gene\",\n",
    "              \"Change\",\n",
    "              \"Protein Effect\",\n",
    "              \"Polymorphism Type\",\n",
    "              \"Minimum\",\n",
    "              \"Maximum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns which should be properly suffixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specific_cols = [i for i in dfs.values()[0] if i not in merge_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce takes it nicely but suffixes are pain\n",
    "#### so let's try with reduce but after renaming columns instead of suffixing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for frame in dfs.values():\n",
    "    for i in specific_cols:\n",
    "        frame.rename(columns={i: \"{} {}\".format(i, frame[\"Strain\"][0])}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_m = reduce(lambda df1, df2: pd.merge(left=df1,\n",
    "                                        right=df2,\n",
    "                                        on=merge_cols,\n",
    "                                        how=\"outer\"),\n",
    "              dfs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some cleaning\n",
    "#### remove cols that consist only of NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_m.dropna(axis=1, how=\"all\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### no need for *Strain* columns now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undes_cols = [i for i in df_m.columns if \"Strain\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_m.drop(columns=undes_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### it is all fine but there is some mess in the columns order..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specific_cols_dict = {}\n",
    "for col_group in specific_cols:\n",
    "    specific_cols_dict[col_group] = [i for i in df_m.columns if col_group in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del specific_cols_dict[\"Strain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ordered_cols = list(itertools.chain.from_iterable(specific_cols_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample W303-nup133-day42',\n",
       " 'Sample BY4741-nup133-day70',\n",
       " 'Sample W303-cog7-day42',\n",
       " 'Sample BY4741-WT-day70',\n",
       " 'Sample W303-WT-day42',\n",
       " 'Variant Frequency W303-nup133-day42',\n",
       " 'Variant Frequency BY4741-nup133-day70',\n",
       " 'Variant Frequency W303-cog7-day42',\n",
       " 'Variant Frequency BY4741-WT-day70',\n",
       " 'Variant Frequency W303-WT-day42',\n",
       " 'Average Quality W303-nup133-day42',\n",
       " 'Average Quality BY4741-nup133-day70',\n",
       " 'Average Quality W303-cog7-day42',\n",
       " 'Average Quality BY4741-WT-day70',\n",
       " 'Average Quality W303-WT-day42']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_m_ord = df_m[merge_cols + ordered_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's make sheet on gdrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sh = gc.create(\"all_merged\", parent_id=gdrive_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "wks=sh.add_worksheet(\"all_merged\")\n",
    "wks.set_dataframe(df_m_ord,\n",
    "                  (1, 1),\n",
    "                  copy_index=True,\n",
    "                  fit=True)\n",
    "sh.del_worksheet(sh.worksheet_by_title(\"Sheet1\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiments with squash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no need for Variation Frequency and so on - let's get rid of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cols that are desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [\"Gene\",\n",
    "        \"Change\",\n",
    "        \"Protein Effect\",\n",
    "        \"Polymorphism Type\",\n",
    "        \"Minimum\",\n",
    "        \"Maximum\"] + [i for i in df_m_ord.columns if \"Sample\" in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cols that are just about SNP occurence in a given sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample W303-nup133-day42',\n",
       " 'Sample BY4741-nup133-day70',\n",
       " 'Sample W303-cog7-day42',\n",
       " 'Sample BY4741-WT-day70',\n",
       " 'Sample W303-WT-day42']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in df_m_ord.columns if \"Sample\" in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_m_small = df_m_ord[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_d = {}\n",
    "sample_names = [i for i in df_m_small.columns if \"Sample\" in i]\n",
    "for gene in df_m_small[\"Gene\"].drop_duplicates():\n",
    "    sample_series_dict = {}\n",
    "    df_g = df_m_small[df_m_small[\"Gene\"] == gene]\n",
    "    for sample in sample_names:\n",
    "        s_gs = df_g.groupby([\"Change\", \"Minimum\", \"Maximum\"]).apply(lambda x: \"-\".join(set(x[sample].dropna().drop_duplicates())))\n",
    "        sample_series_dict[sample] = s_gs\n",
    "    df_gs = pd.DataFrame(sample_series_dict)\n",
    "    out_d[gene] = pd.merge(left=df_g.drop(sample_names, axis=1),\n",
    "                           right=df_gs,\n",
    "                           left_on=[\"Change\", \"Minimum\", \"Maximum\"],\n",
    "                           right_index=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### well, it is *quite* compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 412)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_d[\"COS7 CDS\"]), len(df_m_small[df_m_small[\"Gene\"] == \"COS7 CDS\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now, let's flatten this dict and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_compressed = pd.concat(out_d.values()).sort_values([\"Gene\", \"Minimum\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### let's make sheet on gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sh = gc.create(\"all_merged_compressed\", parent_id=gdrive_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wks=sh.add_worksheet(\"all_merged_compressed\")\n",
    "wks.set_dataframe(df_compressed,\n",
    "                  (1, 1),\n",
    "                  copy_index=True,\n",
    "                  fit=True)\n",
    "sh.del_worksheet(sh.worksheet_by_title(\"Sheet1\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SNPs_sheets_merge]",
   "language": "python",
   "name": "conda-env-SNPs_sheets_merge-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
